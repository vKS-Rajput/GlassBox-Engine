# Glass Box Discovery Engine — Implementation Specification

> **Document Type**: Internal Design Specification  
> **Status**: Awaiting Approval  
> **Constraint**: Zero-budget, self-hosted, explainable, logic-first

---

## 1. System Mental Model

### How the Engine Thinks

The Glass Box Discovery Engine is not a crawler. It does not blindly scrape and index. It is a **reasoning pipeline** that transforms raw web signals into defensible conclusions about client fit.

The engine operates on a single core premise: **No score without citation.** Every recommendation traces back to a specific URL, a specific sentence, a specific timestamp. If the engine cannot explain why it surfaced a lead, it does not surface that lead.

```
Signal → Evidence → Conclusion → Explanation
```

**Stage 1 — Signal Acquisition**: The engine monitors structured feeds (RSS, job boards, funding announcements). These are not arbitrary websites. They are curated sources of ephemeral business events—hiring, fundraising, executive changes—that imply immediate need.

**Stage 2 — Entity Extraction**: A local LLM parses the signal. It does not hallucinate. It extracts structured entities (company name, domain, role) and classifies relevance against predefined criteria. The output is JSON or discard.

**Stage 3 — Enrichment Cascade**: If the entity passes relevance gating, the engine attempts to resolve contact information via a strict waterfall—scrape first, inference second, paid API only as fallback. Every data point is tagged with its source.

**Stage 4 — Deterministic Ranking**: Leads are not sorted by opaque scores. They are grouped by discrete signals (Hiring+Stack+Verified vs. Hiring+Stack vs. Hiring). Ties break by recency, then by confidence.

**Stage 5 — Evidence Ledger Construction**: The lead record is not raw data. It is an evidence object—every field wrapped in provenance metadata. When presented to the user, the system generates a plain-English explanation from this ledger.

### Why This Is a Search Engine

This is not scraping. Scraping is indiscriminate: visit URL, extract DOM, store.

This is search + reasoning:

- **Retrieval**: Curated sources are queried for signals matching predefined intent patterns.
- **Ranking**: Retrieved entities are ranked by deterministic business logic, not by keyword frequency.
- **Explanation**: Every result is accompanied by a machine-generated citation chain.

The engine answers a query: *"Who needs my services right now, and why do I believe that?"* It does not merely return data.

---

## 2. Minimal Core Components

### 2.1 Signal Ingestion

| Attribute | Value |
|-----------|-------|
| **Purpose** | Ingest ephemeral business events from curated, public sources |
| **Inputs** | RSS feeds (Google Alerts, Grants.gov, Greenhouse, Lever); scheduled Firecrawl runs on target company career pages |
| **Outputs** | Raw text payloads with source URL and timestamp |
| **Why it must exist** | Without structured signal acquisition, the engine has nothing to reason over. Ad-hoc scraping is brittle and illegal at scale. |
| **Excluded complexity** | Full-web crawling. Real-time streaming. Login-walled platforms. |

---

### 2.2 Entity Resolution & Semantic Filtering

| Attribute | Value |
|-----------|-------|
| **Purpose** | Determine if a signal is relevant and extract structured entities |
| **Inputs** | Raw text from ingestion; relevance criteria (industry, service type, role keywords) |
| **Outputs** | Structured JSON: `{company, domain, role, relevance_flag, confidence}` or `null` |
| **Why it must exist** | Most signals are noise. Without aggressive filtering, the pipeline floods downstream stages with irrelevant data. LLM inference is expensive—must gate early. |
| **Excluded complexity** | Multi-hop entity resolution. Coreference across documents. Entity linking to external KGs. |

---

### 2.3 Waterfall Enrichment

| Attribute | Value |
|-----------|-------|
| **Purpose** | Resolve contact information for the decision-maker at the identified entity |
| **Inputs** | Company domain; decision-maker role (e.g., "CTO", "Head of Marketing") |
| **Outputs** | Contact record: `{name, email, phone, source_type, source_url, confidence}` |
| **Why it must exist** | Discovery without outreach is useless. Contact resolution is the bridge. Waterfall maximizes fill rate while minimizing paid API spend. |
| **Excluded complexity** | LinkedIn scraping. Social graph traversal. Phone number verification. |

**Waterfall Sequence (strict order):**
1. Scrape `/about` or `/team` page for names and emails (Firecrawl)
2. If name found, generate email permutations (inference)
3. Validate via SMTP handshake (free)
4. If all fail AND lead score ≥ threshold, call paid API (Hunter/Apollo) as last resort

---

### 2.4 Deterministic Scoring

| Attribute | Value |
|-----------|-------|
| **Purpose** | Rank leads by explicit business rules, not black-box models |
| **Inputs** | Evidence ledger for each lead |
| **Outputs** | Tier assignment: `{tier: 1/2/3, sort_keys: [timestamp, confidence]}` |
| **Why it must exist** | Users cannot debug probabilistic scores. Deterministic tiers are explainable and tunable. |
| **Excluded complexity** | ML-based scoring models. Composite numeric scores. Personalization. |

**Tier Logic:**
- **Tier 1**: Hiring Signal + Tech Stack Match + Verified Email
- **Tier 2**: Hiring Signal + Tech Stack Match (no email yet)
- **Tier 3**: Hiring Signal only

Ties within tier: sort by signal recency (newest first), then by email confidence (highest first).

---

### 2.5 Evidence Ledger (SYSTEM INVARIANT)

| Attribute | Value |
|-----------|-------|
| **Purpose** | Enforce provenance as a first-class data contract for every field in the system |
| **Inputs** | Data value + extraction context |
| **Outputs** | Wrapped evidence object: `{value, meta: {confidence, source_type, source_url, timestamp, method}}` |
| **Why it must exist** | This is the core of the "Glass Box" philosophy. Without provenance, the system is a black box. |
| **Excluded complexity** | Version history. Audit trails. Multi-source conflict resolution. |

> **SYSTEM INVARIANT**: No field, attribute, inference, or conclusion may exist in the database without an attached Evidence Object. This is not a guideline. It is an architectural law. Any write operation that attempts to store a raw value without provenance metadata must fail.

**Violation handling**: If any pipeline stage cannot produce a valid Evidence Object for a required field, the lead is rejected. Nullable fields (e.g., `contact_email`) may be absent, but if present, must carry full provenance.

---

### 2.6 Search Layer

| Attribute | Value |
|-----------|-------|
| **Purpose** | Allow users to query the lead corpus by intent, problem, service, freshness, and confidence |
| **Inputs** | User query (structured or natural language) |
| **Outputs** | Ranked list of leads with embedded explanations |
| **Why it must exist** | The engine is not a notification feed. Users must be able to retrieve and filter leads on demand. |
| **Excluded complexity** | Vector similarity search. Semantic embedding indices. Personalized ranking. |

---

## 3. Canonical Logic Pipeline

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        GLASS BOX LOGIC PIPELINE                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────────┐                                                       │
│  │ SIGNAL ACQUISITION│  RSS/Cron triggers → Raw text + URL + timestamp      │
│  └────────┬─────────┘                                                       │
│           │                                                                 │
│           ▼                                                                 │
│  ┌──────────────────┐  Deduplication check (Redis/Postgres)                │
│  │ DEDUPLICATION    │  → If seen before: DISCARD                            │
│  └────────┬─────────┘                                                       │
│           │                                                                 │
│           ▼                                                                 │
│  ┌──────────────────┐  Local LLM (Ollama) + strict JSON schema             │
│  │ ENTITY RESOLUTION│  Prompt: "Is this relevant? Extract entities."       │
│  │ & FILTERING      │  → If confidence < 0.8: DISCARD + log reason          │
│  └────────┬─────────┘  → If valid: {company, domain, role, confidence}     │
│           │                                                                 │
│           ▼                                                                 │
│  ┌──────────────────┐  Tier 1: Scrape → Tier 2: Inference → Tier 3: SMTP   │
│  │ WATERFALL        │  → Tier 4: Paid API (only if score threshold met)    │
│  │ ENRICHMENT       │  → Output: {contact_email, name, source_meta}         │
│  └────────┬─────────┘                                                       │
│           │                                                                 │
│           ▼                                                                 │
│  ┌──────────────────┐  Hierarchical tier assignment, NOT a sum             │
│  │ DETERMINISTIC    │  Rule 1: Hiring present? → Rule 2: Stack match?       │
│  │ SCORING          │  → Rule 3: Contact verified? → Tier 1/2/3            │
│  └────────┬─────────┘                                                       │
│           │                                                                 │
│           ▼                                                                 │
│  ┌──────────────────┐  Wrap every field in provenance metadata             │
│  │ EVIDENCE LEDGER  │  → {value, confidence, source_type, source_url, ts}  │
│  │ CONSTRUCTION     │                                                       │
│  └────────┬─────────┘                                                       │
│           │                                                                 │
│           ▼                                                                 │
│  ┌──────────────────┐  Store to Postgres/SQLite                            │
│  │ SEARCHABLE       │  → Index by: tier, industry, signal_type, recency    │
│  │ RECORD           │  → Expose via query interface                         │
│  └──────────────────┘                                                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Decision Thresholds

| Checkpoint | Threshold | Action if Failed |
|------------|-----------|------------------|
| Deduplication | URL+timestamp hash exists | Discard silently |
| Relevance confidence | < 0.8 | **HARD REJECT** + log reason |
| Entity extraction | Missing company OR domain | **HARD REJECT** |
| Time-sensitive signal | Signal age > 30 days | **HARD REJECT** |
| Waterfall enrichment | All tiers return null | Store lead WITHOUT contact (can retry later) |
| Tier assignment | No hiring signal | **HARD REJECT** |

---

### Relevance Gating (PRECISION GATE)

Relevance gating is the **primary quality control mechanism**. It operates on a binary accept/reject model. There is no "maybe" state.

**Minimum acceptance requirements (ALL must be true):**
1. At least one **time-sensitive intent signal** (hiring, funding, executive change) with age ≤ 30 days
2. At least one **resolvable entity**: company name AND valid domain (resolves to live website)
3. LLM classification confidence ≥ 0.8
4. Valid JSON output from LLM (schema-conformant)

**Hard reject rules (ANY triggers immediate discard):**

| Rule | Trigger | Rationale |
|------|---------|----------|
| R1: No intent signal | Signal text lacks hiring/funding/change keywords | No time-sensitive need |
| R2: Stale signal | Signal timestamp > 30 days old | Opportunity likely closed |
| R3: Missing entity | Company name OR domain not extractable | Cannot enrich or contact |
| R4: Invalid domain | Domain parked, for-sale, or non-resolving | Not a real company |
| R5: Out-of-scope industry | Industry not in target list | Precision over recall |
| R6: Size mismatch | Company size < 2 or > 1000 employees (if detectable) | Budget fit disqualification |
| R7: LLM failure | Invalid JSON or confidence < 0.8 | Unreliable classification |

**Rejection logging (for auditability, not reuse):**

Every rejected signal is logged with:
- `signal_id`: Hash of source URL + timestamp
- `rejection_rule`: Which rule triggered (R1–R7)
- `rejection_reason`: Human-readable explanation
- `raw_signal_snippet`: First 500 chars (for debugging)
- `timestamp`: When rejected

Rejected signals are **not** stored in the leads table. They exist only in a rejection log for pipeline debugging. They are **never** retried or reconsidered.

---

### Confidence Handling

Confidence is **not a composite score**. It is attached per-field and subject to decay:

| Field | Source | Initial Confidence | Decay Rule |
|-------|--------|-------------------|------------|
| `relevance_confidence` | LLM classification | 0.0–1.0 | No decay (static at extraction) |
| `email_confidence` | Waterfall tier | Scraped=0.95, Inferred=0.7, API=0.9 | Decays 0.1 per 30 days |
| `signal_freshness` | Timestamp delta | 1.0 if < 7 days | Drops to 0.5 at 14 days, 0.0 at 30 days |

**Confidence invalidation:**
- If `signal_freshness` reaches 0.0, the lead is moved to an **archive** queue and excluded from active search results
- If `email_confidence` falls below 0.5, the contact is marked `stale` and excluded from outreach suggestions

---

### Where Human Trust Is Preserved

The system never acts autonomously beyond data collection. It:
- Does NOT send emails without human review
- Does NOT infer problems that aren't explicit in signals
- Does NOT generate outreach copy without human sign-off
- Does NOT escalate to paid APIs without explicit configuration

---

## 4. Explainability & Evidence Model (DATA CONTRACT)

### The Evidence Ledger as First-Class Contract

The Evidence Ledger is not a logging mechanism. It is the **canonical data structure** of the system. Every piece of information stored must conform to this contract.

> **INVARIANT**: No field, attribute, inference, or conclusion may exist without an attached Evidence Object. This is enforced at write time. Violations cause hard failures.

---

### Evidence Type Taxonomy

Every Evidence Object must declare its type. There are exactly three valid types:

| Type | Code | Definition | Trust Level | Example |
|------|------|------------|-------------|---------|
| **Direct Observation** | `OBS` | Text scraped from a specific, verifiable URL | Highest | "Hiring Head of Engineering" from `boards.greenhouse.io/acme/jobs/123` |
| **Inference** | `INF` | Value derived from observed data via documented, reproducible rule | Medium | `jane@acme.com` inferred from name "Jane Doe" + domain "acme.com" via pattern `{first}@{domain}` |
| **Third-Party Enrichment** | `API` | Value returned from external service with known data lineage | Variable | Email verified via Hunter.io API response |

**Type-specific requirements:**

| Type | Required Metadata |
|------|------------------|
| `OBS` | `source_url` (verifiable), `timestamp`, `extraction_method` |
| `INF` | `source_evidence_ids` (references to OBS objects used), `inference_rule` (named rule) |
| `API` | `provider_name`, `api_response_id`, `timestamp`, `provider_confidence` |

---

### Evidence Object Schema (Canonical)

```json
{
  "evidence_id": "evt_abc123",
  "field_name": "contact_email",
  "value": "jane@acme.com",
  "type": "INF",
  "meta": {
    "confidence": 0.70,
    "source_evidence_ids": ["evt_xyz789"],
    "inference_rule": "email_permutation_first_at_domain",
    "timestamp": "2026-01-22T10:30:00Z",
    "validated": true,
    "validation_method": "smtp_handshake"
  }
}
```

**Schema enforcement**: Any write that omits `evidence_id`, `type`, `meta.timestamp`, or `meta.confidence` is rejected.

---

### Confidence: Attachment, Decay, and Invalidation

**Attachment**: Confidence is assigned at evidence creation time based on source type and extraction method.

| Source Type | Base Confidence |
|-------------|----------------|
| Direct scrape (OBS) | 0.95 |
| Inference from OBS (INF) | 0.70 |
| Third-party API (API) | Use provider confidence or 0.85 default |

**Decay**: Confidence decays over time for time-sensitive fields.

| Field | Decay Schedule |
|-------|---------------|
| `intent_signal` | -0.25 per 7 days (reaches 0 at 28 days) |
| `contact_email` | -0.10 per 30 days |
| `company_name`, `domain` | No decay |

**Invalidation**: When confidence reaches 0, the evidence is marked `invalidated: true`. Invalidated evidence:
- Remains in the ledger (for audit)
- Is excluded from search results
- Is excluded from tier calculations
- May trigger a re-enrichment attempt if configured

---

### What Happens When Evidence Is Missing or Insufficient

| Scenario | System Response |
|----------|----------------|
| Required field has no Evidence Object | **HARD REJECT**: Lead not stored |
| Evidence Object missing required metadata | **HARD REJECT**: Write fails |
| Confidence below minimum threshold | **HARD REJECT** for required fields; mark `low_confidence` for optional fields |
| Evidence type is unrecognized | **HARD REJECT**: Invalid data |

Required fields: `company_name`, `domain`, `intent_signal`
Optional fields: `contact_email`, `contact_name`

---

### Tracing Conclusions to Sources

Every conclusion the system makes can be reverse-traced via `evidence_id` references:

1. **Why was this lead surfaced?** → `intent_signal.evidence_id` → `source_url`
2. **Why is this a "Tier 1" lead?** → Tier logic references `intent_signal.evidence_id` + `contact_email.evidence_id`
3. **Where did this email come from?** → `contact_email.source_evidence_ids` → original OBS evidence
4. **How fresh is this signal?** → `intent_signal.meta.timestamp` + decay calculation

---

### Explanation Generation

Explanations are **views**, not stored data. They are generated at render time from the Evidence Ledger.

**Template:**
```
Lead {company_name.value} identified via {intent_signal.type}.
Signal: "{intent_signal.value}" (Source: {intent_signal.meta.source_url}).
Contact {contact_name.value} found via {contact_email.type}.
Email: {contact_email.value} (Confidence: {contact_email.meta.confidence}).
```

**Example Output:**
> Lead **Acme Corp** identified via direct observation (OBS).  
> Signal: "Hiring Head of Engineering" (Source: [boards.greenhouse.io/acme/jobs/123](https://boards.greenhouse.io/acme/jobs/123)).  
> Contact **Jane Doe** found via inference (INF) from team page.  
> Email: jane@acme.com (Confidence: 0.70).

> **CRITICAL**: Explanations are ephemeral views. They are never indexed, never stored as separate records, and never used as search input. Only the underlying Evidence Objects are authoritative.

---

## 5. Search Behavior (CONSTRAINED)

### Indexability Rules

Not all data in the system is searchable. The search layer operates only on ledger-backed fields with explicit indexing permission.

**What MAY be indexed:**

| Field | Type | Searchable As |
|-------|------|---------------|
| `company_name.value` | String | Exact match, prefix |
| `domain.value` | String | Exact match |
| `intent_signal.value` | String | Keyword match (extracted keywords only) |
| `intent_signal.meta.timestamp` | DateTime | Range filter (freshness) |
| `contact_email.meta.confidence` | Float | Range filter |
| `tier` | Integer | Exact match, range |
| `intent_signal.type` | Enum | Exact match (hiring, funding, executive_change) |

**What MUST NEVER be indexed:**

| Data Type | Reason |
|-----------|--------|
| AI-generated explanations | Views, not truth. Would create circular reasoning. |
| Raw signal text | Unbounded, unstructured. Would devolve to keyword search. |
| Rejection log contents | Not leads. Debugging data only. |
| Inference rules | Internal logic, not user-facing. |
| Evidence metadata (except confidence, timestamp) | Over-indexing creates noise. |

> **INVARIANT**: If a field is not in the "MAY be indexed" list, it cannot appear in search queries. The search layer rejects queries referencing non-indexed fields.

---

### Structured Reasoning, Not Keyword Matching

This is **not** a keyword search engine. It is a structured query system.

**Keyword search (PROHIBITED)**: "Find leads containing 'React Native'"

**Glass Box search (REQUIRED)**: "Find leads WHERE tier = 1 AND intent_signal.type = 'hiring' AND freshness ≤ 7 days"

The difference:
- Keyword search matches text anywhere, ignoring structure
- Glass Box search matches **evidence-backed attributes** with known provenance

Free-text search is permitted only as a **secondary** filter after structured filters have been applied. It may search within `intent_signal.value` and `company_name.value` only—never across raw text.

---

### Searchable Dimensions (Exhaustive List)

| Dimension | Evidence Field | Query Type | Example |
|-----------|---------------|------------|----------|
| **Intent Type** | `intent_signal.type` | Enum match | `intent_type = 'hiring'` |
| **Freshness** | `intent_signal.meta.timestamp` | Range | `signal_age ≤ 7 days` |
| **Tier** | `tier` | Exact/Range | `tier = 1` |
| **Email Confidence** | `contact_email.meta.confidence` | Range | `email_confidence ≥ 0.8` |
| **Company** | `company_name.value` | Prefix/Exact | `company starts_with 'Acme'` |
| **Has Contact** | `contact_email` | Exists check | `has_contact = true` |

**NOT searchable** (explicitly excluded):
- Inferred problem statements
- Service mappings (future feature, not MVP)
- LLM-generated content of any kind

---

### Deterministic Tie-Breaking

When multiple leads match a query with the same tier:

1. **First sort**: Signal timestamp (newest first)
2. **Second sort**: Email confidence (highest first)
3. **Third sort**: Company name (alphabetical, for stability)

This is **not** a weighted sum. It is a strict priority queue. The user can reason about ordering without understanding a model.

> **PROHIBITED**: No "relevance score" may be computed or displayed. No ML-based ranking. No personalization. The sort order is deterministic and reproducible.

---

### Preventing Drift to RAG or Keyword Search

The research papers warn against black-box AI search. The following constraints prevent architectural drift:

1. **No embedding-based retrieval**: Vector similarity is not used for lead ranking
2. **No LLM in the query path**: LLM is used for extraction, never for search
3. **No semantic expansion**: Query terms are not expanded via synonyms or embeddings
4. **No "smart" relevance**: System does not guess what user wants
5. **Explanations are views**: Never stored, never indexed, never used as search input

---

## 6. MVP Scope Lock

### Included in MVP

| Component | Scope |
|-----------|-------|
| Signal ingestion | 5–10 RSS feeds (job boards, funding news) |
| Entity filtering | Single-industry focus (e.g., "SaaS companies") |
| Waterfall enrichment | Tiers 1–3 only (scrape, inference, SMTP validation) |
| Scoring | 3-tier deterministic model |
| Evidence ledger | Full provenance on all fields |
| Search | CLI or Google Sheets query interface |
| Output | Slack notification or Sheet row per lead |

### Explicitly Excluded (Even If Tempting)

| Feature | Why Excluded |
|---------|--------------|
| Paid API enrichment | Adds cost, removes "zero-budget" constraint |
| Vector semantic search | Over-engineering for < 10k leads |
| Multi-industry classification | Adds complexity without validation |
| Automated outreach | Violates "human trust" principle |
| Real-time streaming | Unnecessary for freelancer scale |
| UI dashboard | Premature; validate logic first |
| Multi-hop entity linking | Requires external KG, adds latency |
| Phone number resolution | Low ROI for email-first outreach |

### Features That Would Damage Explainability

| Feature | Damage |
|---------|--------|
| ML-based lead scoring | Black box, not debuggable |
| Composite numeric scores | "What does 73 mean?" |
| Auto-generated outreach | Trust requires human review |
| Inferred problem statements | LLM hallucination risk |
| Automated send | Legal and ethical exposure |

---

## 7. Trade-offs & Limits

### What the Engine Cannot Detect

| Blind Spot | Reason |
|------------|--------|
| Intent behind login walls | LinkedIn, Crunchbase Pro, etc. are inaccessible |
| Private funding rounds | Not announced publicly |
| Internal hiring (no public job post) | No signal exists |
| Stealth-mode companies | No public presence |
| Budget or timing readiness | Cannot be inferred from public signals |

### Where Inference May Fail

| Failure Mode | Mitigation |
|--------------|------------|
| LLM misclassifies relevance | Confidence threshold (0.8) + human review queue |
| Email permutation wrong | SMTP validation before storage |
| Stale job post (already filled) | Signal freshness decay (< 7 days = fresh) |
| Company domain parked or for sale | Domain validation in enrichment |
| Person left company | No mitigation—static snapshot |

### Explainability vs. Automation Trade-offs

| Automation Level | Explainability Cost |
|------------------|---------------------|
| Fully automated scoring | Impossible to debug |
| Fully automated outreach | Legal risk, ethical breach |
| Fully automated enrichment | Cost overruns, API abuse |
| Human-in-loop at tier assignment | Slows throughput |
| Human-in-loop at outreach | **Required** — no trade-off |

The architecture **intentionally avoids full autonomy**. A solo builder cannot afford to fix automation errors at scale. Human review is cheaper than reputation damage.

### Why This Architecture Rejects Full Autonomy

1. **Legal exposure**: Automated outreach to EU contacts without GDPR compliance is liability.
2. **Reputation risk**: One bad cold email can burn a domain.
3. **Cost risk**: Runaway API calls can exceed zero-budget constraint.
4. **Trust risk**: Clients who discover they were "AI-scraped" lose trust.

The engine is a **force multiplier**, not a replacement for judgment.

---

## 8. Execution Order (CORRECTED)

### Phase 0: Environment Setup
- [ ] Install Docker Desktop
- [ ] Deploy n8n container (local)
- [ ] Deploy Ollama container with Mistral 7B or Llama 3 8B
- [ ] Configure `host.docker.internal` networking between containers
- [ ] Deploy Firecrawl (self-hosted or free tier)
- [ ] Provision PostgreSQL or SQLite

### Phase 1: Evidence Ledger Schema (MUST BE FIRST)
> **Rationale**: The Evidence Ledger is the system invariant. All downstream components write to it. It must be designed and locked before any data flows.

- [ ] Define Evidence Object schema (JSON structure with required fields)
- [ ] Define evidence type taxonomy (OBS, INF, API) with required metadata per type
- [ ] Implement database schema with Evidence columns (not raw strings)
- [ ] Implement write-time validation: reject any write missing required Evidence metadata
- [ ] **STOP. Validate**: Attempt to write a raw string. Does it fail? Attempt to write Evidence without `evidence_id`. Does it fail?

### Phase 2: Signal Acquisition (validate input structure)
- [ ] Define 5 target RSS feeds (Greenhouse, Lever, TechCrunch, Grants.gov, Google Alerts)
- [ ] Build n8n workflow: RSS trigger → raw text extraction → deduplication check
- [ ] Store raw signals as Evidence Objects (type=OBS) with full metadata
- [ ] **STOP. Validate**: Are signals stored with valid Evidence structure? Can you query by source_url?

### Phase 3: Relevance Gating (validate precision before volume)
- [ ] Write relevance prompt with strict JSON schema enforcement
- [ ] Implement hard reject rules (R1–R7) as explicit checks
- [ ] Build rejection log table (separate from leads)
- [ ] Build n8n node: Ollama call → JSON parse → confidence check → reject or pass
- [ ] **STOP. Validate**: Feed 20 signals. How many pass? How many reject per rule? Is precision > 80%?

### Phase 4: Waterfall Enrichment (validate data quality before scaling)
- [ ] Build Tier 1: Firecrawl scrape → produce Evidence Object (type=OBS)
- [ ] Build Tier 2: Email permutation → produce Evidence Object (type=INF) with `source_evidence_ids`
- [ ] Build Tier 3: SMTP validation → update Evidence Object with `validated: true`
- [ ] Wire waterfall: if null → next tier; output must always be Evidence Object or null
- [ ] **STOP. Validate**: Every contact has Evidence Object? Can you trace email back to original OBS?

### Phase 5: Scoring & Storage (validate logic before exposure)
- [ ] Implement tier assignment logic referencing Evidence Objects
- [ ] Store complete leads with all Evidence columns populated
- [ ] Implement confidence decay scheduler (optional for MVP, but design now)
- [ ] **STOP. Validate**: Query a lead. Can you explain every field from Evidence Ledger alone?

### Phase 6: Search & Presentation (validate usefulness last)
- [ ] Build query interface with ONLY indexed fields (per Section 5 constraints)
- [ ] Implement explanation template (view generation from Evidence)
- [ ] Output to Slack channel or Sheet
- [ ] **STOP. Validate**: Is free-text search disabled or secondary? Does a real lead make sense?

### What Should NOT Be Built Until Users Validate

| Feature | Precondition |
|---------|--------------|
| Paid API fallback (Tier 4) | Fill rate < 30% after 50 leads |
| Multi-industry classification | MVP is validated on one industry |
| Automated outreach drafts | User confirms they want this |
| UI dashboard | CLI/Sheets workflow proves inadequate |
| Vector search | Lead corpus exceeds 10k |
| Real-time monitoring | Batch mode proves too slow |

---

## Verification Plan

### Automated Tests
This is an architecture-only document. No code is produced. Verification will occur during implementation:

1. **Signal ingestion test**: Run RSS trigger, confirm raw signals stored with correct source URLs
2. **LLM output test**: Feed 10 sample signals, confirm valid JSON output with >80% accuracy
3. **Waterfall test**: Provide 5 known company domains, verify email fill rate ≥ 60%
4. **Evidence ledger test**: Query database, confirm every field has attached metadata
5. **Explanation generation test**: Render 3 leads, confirm plain-English output matches template

### Manual Verification
1. **User review**: Present 10 leads to user and ask: "Would you reach out to this lead? Why or why not?"
2. **Provenance audit**: Pick any lead field, user traces it back to source URL manually
3. **Tier logic review**: User confirms tier assignments match their mental model of lead quality

---

## Summary

This specification operationalizes the Glass Box philosophy from the research papers into a buildable system:

- **Signals, not scraping**: Curated feeds, not full-web crawling
- **Evidence, not scores**: Every datum traced to source
- **Tiers, not probabilities**: Deterministic, debuggable ranking
- **Explanation by default**: No lead without citation
- **Human in the loop**: No automation beyond collection

Build in order. Validate at each phase. Do not proceed until the current stage is trustworthy.
